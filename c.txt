# ------------------------------
# Simple Linear SVM from Scratch
# ------------------------------

# Step 1: Dataset (2D list)
# Each entry = [x1, x2, label]
# label is +1 or -1
data = [
    [2, 3, 1],
    [1, 1, -1],
    [2, 1, -1],
    [3, 2, 1],
    [2, 4, 1],
    [1, 2, -1]
]

# Step 2: Separate features and labels
X = []
y = []
for row in data:
    X.append([row[0], row[1]])
    y.append(row[2])

# Step 3: Initialize parameters
w = [0.0, 0.0]   # weights
b = 0.0           # bias
lr = 0.001        # learning rate
C = 1.0           # regularization constant
epochs = 10000    # number of iterations

# Step 4: Training using Gradient Descent
for epoch in range(epochs):
    for i in range(len(X)):
        x1, x2 = X[i][0], X[i][1]
        yi = y[i]
        
        # Margin check
        condition = yi * (w[0]*x1 + w[1]*x2 + b)
        
        if condition >= 1:
            # No misclassification, update only regularization term
            w[0] -= lr * (2 * w[0])
            w[1] -= lr * (2 * w[1])
        else:
            # Misclassified â†’ apply hinge loss gradient
            w[0] -= lr * (2 * w[0] - C * yi * x1)
            w[1] -= lr * (2 * w[1] - C * yi * x2)
            b -= lr * (-C * yi)

# Step 5: Prediction function
def predict(x):
    value = w[0]*x[0] + w[1]*x[1] + b
    return 1 if value >= 0 else -1

# Step 6: Test on training data
correct = 0
for i in range(len(X)):
    pred = predict(X[i])
    if pred == y[i]:
        correct += 1
    print("Input:", X[i], "Actual:", y[i], "Predicted:", pred)

accuracy = correct / len(X)
print("\nFinal weights:", w)
print("Final bias:", b)
print("Training accuracy:", accuracy)

# Step 7: Test on new unseen samples
test_points = [[1,3], [3,3], [2,2], [4,1]]
print("\nTesting on new samples:")
for t in test_points:
    print("Point:", t, "Predicted class:", predict(t))